import sys
import json
import argparse
import logging
from datetime import datetime, timedelta
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# åˆ†ç‰‡å‚æ•°
MAX_ENTRIES_PER_SHARD = 10000  # æ¯ä¸ªåˆ†ç‰‡æœ€å¤šå­˜å‚¨10000æ¡è®°å½•
SHARD_DIR = '../dataBase/'     # åˆ†ç‰‡å­˜å‚¨ç›®å½•
FORMAT_SHARDS = True           # æ˜¯å¦æ ¼å¼åŒ–åˆ†ç‰‡æ–‡ä»¶ï¼ˆTrue: å¯è¯»æ ¼å¼ï¼ŒFalse: é«˜æ€§èƒ½ç´§å‡‘æ ¼å¼ï¼‰

def get_entry_id(entry):
    """ç”Ÿæˆåª’ä½“æ¡ç›®çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
    return f"{entry['file_name']}_{entry['user']['screenName']}_{entry['media_type']}"

def get_shard_files():
    """è·å–æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶è·¯å¾„"""
    if not os.path.exists(SHARD_DIR):
        os.makedirs(SHARD_DIR)
    files = []
    for filename in os.listdir(SHARD_DIR):
        if filename.startswith("processed_entries_") and filename.endswith(".json"):
            files.append(os.path.join(SHARD_DIR, filename))
    return files

def parse_shard_number(file_path):
    """ä»æ–‡ä»¶åä¸­è§£æåˆ†ç‰‡ç¼–å·"""
    basename = os.path.basename(file_path)
    parts = basename.split('_')[-1].split('.')[-2].split('-')
    return int(parts[-1]) if len(parts) > 0 else 0

def get_max_shard_number(year_month):
    """è·å–æŒ‡å®šå¹´æœˆçš„æœ€å¤§åˆ†ç‰‡ç¼–å·"""
    max_num = 0
    for file_path in get_shard_files():
        if f"_{year_month}-" in file_path:
            num = parse_shard_number(file_path)
            if num > max_num:
                max_num = num
    return max_num

def save_entry(entry_id):
    """å°†æ¡ç›®IDä¿å­˜åˆ°åˆ†ç‰‡æ–‡ä»¶"""
    year_month = datetime.now().strftime("%Y-%m")
    current_max_shard = get_max_shard_number(year_month)
    current_shard = current_max_shard + 1

    candidate_path = os.path.join(
        SHARD_DIR,
        f"processed_entries_{year_month}-{current_max_shard:04d}.json"
    )

    if os.path.exists(candidate_path):
        try:
            with open(candidate_path, 'r') as f:
                entries = json.load(f)
            if len(entries) < MAX_ENTRIES_PER_SHARD:
                entries.append(entry_id)
                with open(candidate_path, 'w') as f:
                    if FORMAT_SHARDS:
                        json.dump(entries, f, indent=2)
                    else:
                        json.dump(entries, f)
                return candidate_path
        except json.JSONDecodeError:
            logger.warning(f"è­¦å‘Šï¼šåˆ†ç‰‡æ–‡ä»¶æŸåï¼Œå°è¯•é‡å†™ï¼š{candidate_path}")
            with open(candidate_path, 'w') as f:
                if FORMAT_SHARDS:
                    json.dump([entry_id], f, indent=2)
                else:
                    json.dump([entry_id], f)
            return candidate_path

    # åˆ›å»ºæ–°åˆ†ç‰‡
    shard_filename = f"processed_entries_{year_month}-{current_shard:04d}.json"
    shard_path = os.path.join(SHARD_DIR, shard_filename)
    with open(shard_path, 'w') as f:
        if FORMAT_SHARDS:
            json.dump([entry_id], f, indent=2)
        else:
            json.dump([entry_id], f)
    return shard_path

def load_processed_entries():
    """åŠ è½½æ‰€æœ‰å·²å¤„ç†çš„æ¡ç›®IDé›†åˆ"""
    processed = set()
    for file_path in get_shard_files():
        try:
            with open(file_path, 'r') as f:
                entries = json.load(f)
                if isinstance(entries, list):
                    processed.update(entries)
                else:
                    logger.warning(f"è­¦å‘Šï¼šåˆ†ç‰‡æ–‡ä»¶ {file_path} æ ¼å¼ä¸æ­£ç¡®ï¼ˆéåˆ—è¡¨ç±»å‹ï¼‰ï¼Œå·²è·³è¿‡")
        except json.JSONDecodeError:
            logger.warning(f"è­¦å‘Šï¼šåˆ†ç‰‡æ–‡ä»¶æŸåï¼š{file_path}ï¼Œå·²è·³è¿‡")
        except Exception as e:
            logger.error(f"é”™è¯¯è¯»å–åˆ†ç‰‡æ–‡ä»¶ {file_path}: {str(e)}ï¼Œå·²è·³è¿‡")
    return processed

def main(data_file, config_file, output_file):

    logger.info("ğŸ¬ å¼€å§‹å¤„ç†æ¨æ–‡")
    logger.info(f"ğŸ“ JSONè·¯å¾„: {data_file}")
    logger.info(f"ğŸ“¥ å¯¼å‡ºç›®å½•: {output_file}")

    # ç¡®ä¿åˆ†ç‰‡ç›®å½•å­˜åœ¨
    if not os.path.exists(SHARD_DIR):
        os.makedirs(SHARD_DIR)

    # è¯»å–é…ç½®æ–‡ä»¶
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        users_to_query = [user["legacy"]["screenName"] for user in config]
        if not users_to_query:
            logger.warning("é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šè¦æŸ¥è¯¢çš„ç”¨æˆ·ï¼")
            return
    except FileNotFoundError:
        logger.error(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼")
        return
    except json.JSONDecodeError:
        logger.error(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ {config_file} æ ¼å¼ä¸æ­£ç¡®ï¼")
        return

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ‰€æœ‰åˆ†ç‰‡ä¸­çš„æ¡ç›®ID
    processed_entries = load_processed_entries()

    # è¯»å–å¹¶è§£æè¾“å…¥æ•°æ®æ–‡ä»¶
    user_data = {}
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            users_list = json.load(f)

            for user_entry in users_list:
                user = user_entry.get("user", {})
                screen_name = user.get("screenName")
                if not screen_name:
                    logger.warning(f"è­¦å‘Šï¼šç”¨æˆ·å¯¹è±¡ç¼ºå°‘screenNameå­—æ®µï¼Œè·³è¿‡ {user_entry}")
                    continue

                if screen_name not in user_data:
                    user_data[screen_name] = {
                        "name": user.get("name", "N/A"),
                        "entries": []
                    }

                entry = {
                    "fullText": user_entry.get("fullText", ""),
                    "publishTime": user_entry.get("publishTime", ""),
                    "images": list(user_entry.get("images", [])),
                    "videos": list(user_entry.get("videos", []))
                }
                user_data[screen_name]["entries"].append(entry)

    except FileNotFoundError:
        logger.error(f"é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ {data_file} æœªæ‰¾åˆ°ï¼")
        return
    except json.JSONDecodeError as e:
        logger.error(f"é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼{str(e)}")
        return

    # ç”Ÿæˆæ–°æ¡ç›®ï¼ˆå…¨å±€å»é‡ï¼‰
    new_entries = []
    current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

    for target in users_to_query:
        user_info = user_data.get(target)
        if not user_info:
            continue

        for entry in user_info["entries"]:
            full_text = entry["fullText"]
            publish_time = entry["publishTime"]
            for media_type in ["images", "videos"]:
                media_list = entry.get(media_type, [])
                for media_url in media_list:
                    filename = media_url.split("?")[0].split("/")[-1]
                    entry_id = f"{filename}_{target}_{media_type}"

                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡ï¼ˆå…¨å±€å»é‡ï¼‰
                    if entry_id in processed_entries:
                        continue

                    # åˆ›å»ºæ–°æ¡ç›®ï¼Œåˆå§‹çŠ¶æ€
                    new_entry = {
                        "file_name": filename,
                        "user": {
                            "screenName": target,
                            "name": user_info["name"]
                        },
                        "media_type": media_type,
                        "url": media_url,
                        "read_time": current_time,  # ä»…é¦–æ¬¡å¤„ç†æ—¶è®°å½•
                        "is_uploaded": False,
                        "upload_info": {},
                        "is_downloaded": False,
                        "download_info": {},
                        "fullText": full_text,
                        "publishTime": publish_time
                    }

                    new_entries.append(new_entry)
                    # æ ‡è®°ä¸ºå·²å¤„ç†
                    save_entry(entry_id)

    # åˆå¹¶æ–°æ—§è¾“å‡ºæ–‡ä»¶ï¼ˆä»…æ·»åŠ æ–°æ¡ç›®ï¼‰
    try:
        existing_entries = []
        if os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_entries = json.load(f)
    except Exception as e:
        logger.warning(f"è­¦å‘Šï¼šè¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        existing_entries = []

    # åˆå¹¶é€»è¾‘ï¼šä¿ç•™æ‰€æœ‰ç°æœ‰æ¡ç›®ï¼Œä»…æ·»åŠ æ–°æ¡ç›®
    merged_entries = existing_entries.copy()
    existing_entry_ids = {get_entry_id(e) for e in existing_entries}

    new_count = 0
    for new_entry in new_entries:
        entry_id = get_entry_id(new_entry)
        if entry_id not in existing_entry_ids:
            merged_entries.append(new_entry)
            new_count += 1

    # ä¿®æ”¹æ’åºé€»è¾‘ï¼šå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
    merged_entries.sort(key=lambda x: x.get('publishTime', '1970-01-01T00:00:00'))

    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            if FORMAT_SHARDS:
                json.dump(merged_entries, f, indent=2, ensure_ascii=False)
            else:
                json.dump(merged_entries, f, ensure_ascii=False)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ–°å¢æ¡ç›®æ•°: {new_count}")
    except Exception as e:
        logger.error(f"å†™å…¥æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")

if __name__ == "__main__":
    if len(sys.argv) == 4:  # è„šæœ¬å + 3ä¸ªå‚æ•°
        data_file = os.path.normpath(sys.argv[1])
        config_file = os.path.normpath(sys.argv[2])
        output_file = os.path.normpath(sys.argv[3])
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è·³è¿‡
        if os.path.exists(data_file):
            main(data_file, config_file, output_file)
        else:
            logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ï¼š{data_file}")
    elif len(sys.argv) == 2:  # è„šæœ¬å + æ•°æ®æ–‡ä»¶
        data_file = os.path.normpath(sys.argv[1])
        current_date = datetime.now()
        config_file = os.path.normpath("../config/followingUser.json")

        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.normpath(
            f"../output/"
            f"{current_date:%Y-%m}/{current_date:%Y-%m-%d}.json"
        )

        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è·³è¿‡
        if os.path.exists(data_file):
            main(data_file, config_file, output_file)
        else:
            logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ï¼š{data_file}")
    elif len(sys.argv) == 1:  # ä»…è„šæœ¬å
        # é»˜è®¤å¤„ç†ä»Šå¤©å’Œæ˜¨å¤©
        current_date = datetime.now()
        config_file = os.path.normpath("../config/followingUser.json")

        for day_offset in range(8):  # ä¸€å‘¨
            target_date = current_date - timedelta(days=day_offset)

            # æ¨æ–‡æ•°æ®æ–‡ä»¶è·¯å¾„
            data_file = os.path.normpath(
                f"../../TypeScript/tweets/"
                f"{target_date:%Y-%m}/{target_date:%Y-%m-%d}.json"
            )

            # è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_file = os.path.normpath(
                f"../output/"
                f"{target_date:%Y-%m}/{target_date:%Y-%m-%d}.json"
            )

            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è·³è¿‡
            if os.path.exists(data_file):
                main(data_file, config_file, output_file)
            else:
                logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ï¼š{data_file}")
    else:
        logger.error("é”™è¯¯ï¼šå‚æ•°æ•°é‡ä¸æ­£ç¡®")
        logger.info("ä½¿ç”¨æ–¹æ³•ï¼špython X-Bot.py [<æ¨æ–‡æ•°æ®æ–‡ä»¶> <é…ç½®æ–‡ä»¶> <è¾“å‡ºæ–‡ä»¶>]")
        logger.info("ç¤ºä¾‹ï¼š")
        logger.info("å¸¦å‚æ•°ï¼špython X-Bot.py ../../TypeScript/tweets/2000-01/2000-01-01.json ../config/followingUser.json ../output/2000-01/2000-01-01.json")
        logger.info("ç”¨é»˜è®¤ï¼špython X-Bot.py")
        sys.exit(1)
