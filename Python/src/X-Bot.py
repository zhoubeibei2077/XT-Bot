import sys
import json
import os
from datetime import datetime, timedelta
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°æ¨¡å—æœç´¢è·¯å¾„
_project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(_project_root))
from utils.log_utils import LogUtils


# --------------------
# é…ç½®åŒº
# --------------------
class Config:
    # åˆ†ç‰‡é…ç½®
    MAX_ENTRIES_PER_SHARD = 10000  # å•ä¸ªåˆ†ç‰‡æœ€å¤§æ¡ç›®æ•°
    SHARD_DIR = "../dataBase/"  # åˆ†ç‰‡å­˜å‚¨ç›®å½•
    FORMAT_SHARDS = True  # æ˜¯å¦æ ¼å¼åŒ–åˆ†ç‰‡æ–‡ä»¶
    SHARD_PREFIX = "processed_entries_"

    # è·¯å¾„é…ç½®
    DEFAULT_INPUT_DIR = "../../TypeScript/tweets/"  # é»˜è®¤è¾“å…¥ç›®å½•
    DEFAULT_OUTPUT_DIR = "../output/"  # é»˜è®¤è¾“å‡ºç›®å½•

    # æ—¥æœŸæ ¼å¼
    DATE_FORMAT = "%Y-%m-%dT%H:%M:%S"  # æ—¶é—´æˆ³æ ¼å¼
    YEAR_MONTH_DAY = "%Y-%m-%d"  # å¹´æœˆæ—¥æ ¼å¼
    YEAR_MONTH = "%Y-%m"  # å¹´æœˆæ ¼å¼


# å¼•å…¥æ—¥å¿—æ¨¡å—
logger = LogUtils().get_logger()
logger.info("ğŸ”„ X-Bot åˆå§‹åŒ–å®Œæˆ")


# --------------------
# åˆ†ç‰‡ç®¡ç†å™¨
# --------------------
class ShardManager:
    """ç®¡ç†å·²å¤„ç†æ¡ç›®çš„åˆ†ç‰‡å­˜å‚¨"""

    def __init__(self):
        self._ensure_shard_dir()

    def _ensure_shard_dir(self):
        """ç¡®ä¿åˆ†ç‰‡ç›®å½•å­˜åœ¨"""
        if not os.path.exists(Config.SHARD_DIR):
            os.makedirs(Config.SHARD_DIR)
            logger.info(f"ğŸ“ åˆ›å»ºåˆ†ç‰‡ç›®å½•: {Config.SHARD_DIR}")

    def get_current_shard_info(self):
        """è·å–å½“å‰åˆ†ç‰‡ä¿¡æ¯"""
        year_month = datetime.now().strftime(Config.YEAR_MONTH)
        max_shard = self._get_max_shard_number(year_month)
        return {
            "year_month": year_month,
            "current_max": max_shard,
            "next_shard": max_shard + 1
        }

    def _get_max_shard_number(self, year_month):
        """è·å–æŒ‡å®šå¹´æœˆæœ€å¤§åˆ†ç‰‡å·"""
        max_num = 0
        for file_path in self._list_shard_files():
            if f"_{year_month}-" in file_path:
                num = self._parse_shard_number(file_path)
                max_num = max(max_num, num)
        return max_num

    def _list_shard_files(self):
        """åˆ—å‡ºæ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶"""
        return [
            os.path.join(Config.SHARD_DIR, f)
            for f in os.listdir(Config.SHARD_DIR)
            if f.startswith(Config.SHARD_PREFIX) and f.endswith(".json")
        ]

    @staticmethod
    def _parse_shard_number(file_path):
        """ä»æ–‡ä»¶è·¯å¾„è§£æåˆ†ç‰‡ç¼–å·"""
        filename = os.path.basename(file_path)
        return int(filename.split("-")[-1].split(".")[0])

    def save_entry_id(self, entry_id):
        """ä¿å­˜æ¡ç›®IDåˆ°åˆé€‚çš„åˆ†ç‰‡"""
        shard_info = self.get_current_shard_info()
        candidate_path = self._build_shard_path(shard_info["year_month"], shard_info["current_max"])

        # å°è¯•å†™å…¥ç°æœ‰åˆ†ç‰‡
        if os.path.exists(candidate_path):
            try:
                with open(candidate_path, "r+") as f:
                    entries = json.load(f)
                    if len(entries) < Config.MAX_ENTRIES_PER_SHARD:
                        entries.append(entry_id)
                        f.seek(0)
                        json.dump(entries, f, indent=2 if Config.FORMAT_SHARDS else None)
                        logger.debug(f"ğŸ“¥ æ¡ç›® {entry_id} å·²å†™å…¥ç°æœ‰åˆ†ç‰‡: {candidate_path}")
                        return candidate_path
            except json.JSONDecodeError:
                logger.warning("ğŸ”„ æ£€æµ‹åˆ°æŸååˆ†ç‰‡ï¼Œå°è¯•ä¿®å¤...")
                return self._handle_corrupted_shard(candidate_path, entry_id)

        # åˆ›å»ºæ–°åˆ†ç‰‡
        new_path = self._build_shard_path(shard_info["year_month"], shard_info["next_shard"])
        self._write_shard(new_path, [entry_id])
        logger.info(f"âœ¨ åˆ›å»ºæ–°åˆ†ç‰‡: {new_path}")
        return new_path

    def _build_shard_path(self, year_month, shard_number):
        """æ„å»ºåˆ†ç‰‡æ–‡ä»¶è·¯å¾„"""
        return os.path.join(
            Config.SHARD_DIR,
            f"{Config.SHARD_PREFIX}{year_month}-{shard_number:04d}.json"
        )

    def _handle_corrupted_shard(self, path, entry_id):
        """å¤„ç†æŸåçš„åˆ†ç‰‡æ–‡ä»¶"""
        try:
            self._write_shard(path, [entry_id])
            logger.warning(f"âœ… æˆåŠŸä¿®å¤æŸååˆ†ç‰‡: {path}")
            return path
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤åˆ†ç‰‡å¤±è´¥: {str(e)}")
            raise

    def _write_shard(self, path, data):
        """å†™å…¥åˆ†ç‰‡æ–‡ä»¶"""
        with open(path, "w") as f:
            json.dump(data, f, indent=2 if Config.FORMAT_SHARDS else None)

    def load_processed_entries(self):
        """åŠ è½½æ‰€æœ‰å·²å¤„ç†æ¡ç›®"""
        processed = set()
        for file_path in self._list_shard_files():
            try:
                with open(file_path, "r") as f:
                    entries = json.load(f)
                    processed.update(entries)
                    logger.debug(f"ğŸ“– åŠ è½½åˆ†ç‰‡: {file_path} (æ¡ç›®æ•°: {len(entries)})")
            except Exception as e:
                logger.warning(f"âš ï¸ è·³è¿‡æŸååˆ†ç‰‡ {file_path}: {str(e)}")
        logger.info(f"ğŸ” å·²åŠ è½½å†å²æ¡ç›®æ€»æ•°: {len(processed)}")
        return processed


# --------------------
# æ¡ç›®å¤„ç†å™¨
# --------------------
class EntryProcessor:
    """å¤„ç†æ¨æ–‡æ¡ç›®ä¸­çš„åª’ä½“èµ„æº"""

    @staticmethod
    def generate_entry_id(filename, username, media_type):
        """ç”Ÿæˆå”¯ä¸€æ¡ç›®ID"""
        return f"{filename}_{username}_{media_type}"

    @staticmethod
    def create_entry_template(filename, user_info, media_type, url):
        """åˆ›å»ºæ ‡å‡†æ¡ç›®æ¨¡æ¿"""
        return {
            "tweet_id": "",
            "file_name": filename,
            "user": {
                "screen_name": user_info["screen_name"],
                "name": user_info.get("name", "N/A")
            },
            "media_type": media_type,
            "url": url,
            "read_time": datetime.now().strftime(Config.DATE_FORMAT),
            "is_uploaded": False,
            "upload_info": {},
            "is_downloaded": False,
            "download_info": {},
            "full_text": "",
            "publish_time": ""
        }

    def process_entry(self, entry, user_info, processed_ids):
        """å¤„ç†å•ä¸ªæ¨æ–‡æ¡ç›®"""
        new_entries = []

        # æå–æ¡ç›®ä¸­çš„ tweet_id
        tweet_id = self._extract_tweet_id(entry.get("tweet_url", ""))

        # å¤„ç†æ™®é€šåª’ä½“
        new_entries.extend(self._process_media(entry, user_info, processed_ids, "images"))
        new_entries.extend(self._process_media(entry, user_info, processed_ids, "videos"))

        # å¤„ç†ç‰¹æ®Šé“¾æ¥
        new_entries.extend(self._process_special_urls(entry, user_info, processed_ids))

        # è¡¥å……å…ƒæ•°æ®
        for e in new_entries:
            e.update({
                "tweet_id": tweet_id,
                "full_text": entry.get("full_text", ""),
                "publish_time": entry.get("publish_time", "")
            })

        return new_entries

    def _process_media(self, entry, user_info, processed_ids, media_type):
        """å¤„ç†å›¾ç‰‡/è§†é¢‘ç±»åª’ä½“"""
        entries = []
        for url in entry.get(media_type, []):
            filename = self._extract_filename(url)
            entry_id = self.generate_entry_id(filename, user_info["screen_name"], media_type)

            if entry_id in processed_ids:
                continue

            new_entry = self.create_entry_template(filename, user_info, media_type, url)
            entries.append(new_entry)
            logger.debug(f"ğŸ“· å‘ç°æ–°{media_type}æ¡ç›®: {filename}")

        return entries

    def _process_special_urls(self, entry, user_info, processed_ids):
        """å¤„ç†å¹¿æ’­/ç©ºé—´é“¾æ¥"""
        entries = []
        for url in entry.get("expand_urls", []):
            media_type = self._detect_media_type(url)
            if not media_type:
                continue

            filename = self._extract_filename(url)
            entry_id = self.generate_entry_id(filename, user_info["screen_name"], media_type)

            if entry_id in processed_ids:
                continue

            new_entry = self.create_entry_template(filename, user_info, media_type, url)
            entries.append(new_entry)
            logger.debug(f"ğŸ”— å‘ç°ç‰¹æ®Šé“¾æ¥: {media_type} - {filename}")

        return entries

    @staticmethod
    def _extract_tweet_id(tweet_url):
        """ä»æ¨æ–‡URLæå–å”¯ä¸€ID"""
        if not tweet_url:
            return ""

        # æŸ¥æ‰¾/status/åçš„éƒ¨åˆ†ä½œä¸ºæ¨æ–‡ID
        parts = tweet_url.split("/status/")
        if len(parts) > 1:
            # è·å–IDéƒ¨åˆ†ï¼Œå¹¶ç§»é™¤å¯èƒ½å­˜åœ¨çš„æŸ¥è¯¢å‚æ•°
            tweet_id = parts[1].split("?")[0].split("/")[0]
            # ç¡®ä¿IDæ˜¯çº¯æ•°å­—
            if tweet_id.isdigit():
                return tweet_id

        return ""

    @staticmethod
    def _extract_filename(url):
        """ä»URLæå–æ–‡ä»¶å"""
        return url.split("?")[0].split("/")[-1]

    @staticmethod
    def _detect_media_type(url):
        """è¯†åˆ«é“¾æ¥ç±»å‹"""
        if "/broadcasts/" in url:
            return "broadcasts"
        if "/spaces/" in url:
            return "spaces"
        return None


# --------------------
# æ–‡ä»¶ç®¡ç†å™¨
# --------------------
class FileManager:
    """å¤„ç†æ–‡ä»¶IOæ“ä½œ"""

    @staticmethod
    def load_json(path):
        """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            logger.info(f"ğŸ“‚ æˆåŠŸåŠ è½½æ–‡ä»¶: {path}")
            return data
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
            raise
        except json.JSONDecodeError:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {path}")
            raise

    @staticmethod
    def save_output(data, output_path):
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
        output_dir = os.path.dirname(output_path)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ è¾“å‡ºå·²ä¿å­˜è‡³: {output_path}")


# --------------------
# æ ¸å¿ƒæµç¨‹
# --------------------
class XBotCore:
    """ä¸»å¤„ç†é€»è¾‘"""

    def __init__(self):
        self.shard_manager = ShardManager()
        self.entry_processor = EntryProcessor()
        self.file_manager = FileManager()
        self.processed_ids = self.shard_manager.load_processed_entries()

    def process_single_day(self, data_path, output_path):
        """å¤„ç†å•æ—¥æ•°æ®"""
        logger.info(f"\n{'-' * 40}\nğŸ” å¼€å§‹å¤„ç†: {os.path.basename(data_path)}")

        # åŠ è½½æ•°æ®
        raw_data = self.file_manager.load_json(data_path)
        user_data = self._organize_user_data(raw_data)

        # å¤„ç†æ¡ç›®
        all_new_entries = []
        # éå†æ‰€æœ‰ç”¨æˆ·
        for username in user_data:

            user_info = user_data[username]

            user_entries = []
            for entry in user_info["entries"]:
                user_entries.extend(self.entry_processor.process_entry(entry, user_info, self.processed_ids))

            # ä¿å­˜æ–°æ¡ç›®ID
            for entry in user_entries:
                entry_id = EntryProcessor.generate_entry_id(
                    entry["file_name"],
                    entry["user"]["screen_name"],
                    entry["media_type"]
                )
                self.shard_manager.save_entry_id(entry_id)

            all_new_entries.extend(user_entries)

        # åˆå¹¶è¾“å‡º
        final_output = self._merge_output(output_path, all_new_entries)
        self.file_manager.save_output(final_output, output_path)
        logger.info(f"ğŸ‰ æœ¬æ—¥å¤„ç†å®Œæˆï¼æ–°å¢æ¡ç›®: {len(all_new_entries)}\n{'-' * 40}\n")
        return len(all_new_entries)

    def _organize_user_data(self, raw_data):
        """é‡ç»„ç”¨æˆ·æ•°æ®ç»“æ„"""
        organized = {}
        for item in raw_data:
            user = item.get("user", {})
            username = user.get("screenName")
            if not username:
                continue

            if username not in organized:
                organized[username] = {
                    "screen_name": username,
                    "name": user.get("name", "N/A"),
                    "entries": []
                }

            organized[username]["entries"].append({
                "tweet_url": item.get("tweetUrl", ""),
                "full_text": item.get("fullText", ""),
                "publish_time": item.get("publishTime", ""),
                "images": item.get("images", []),
                "videos": item.get("videos", []),
                "expand_urls": item.get("expandUrls", [])
            })
        return organized

    def _merge_output(self, output_path, new_entries):
        """åˆå¹¶æ–°æ—§è¾“å‡ºæ–‡ä»¶"""
        existing = []
        if os.path.exists(output_path):
            existing = self.file_manager.load_json(output_path)
            logger.info(f"ğŸ”„ åˆå¹¶ç°æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå·²æœ‰æ¡ç›®: {len(existing)}")

        existing_ids = {self._get_entry_id(e) for e in existing}
        merged = existing.copy()
        added = 0

        for entry in new_entries:
            entry_id = self._get_entry_id(entry)
            if entry_id not in existing_ids:
                merged.append(entry)
                added += 1

        merged.sort(key=lambda x: x.get("publish_time", ""))
        logger.info(f"ğŸ†• æ–°å¢æ¡ç›®: {added} | åˆå¹¶åæ€»æ•°: {len(merged)}")
        return merged

    @staticmethod
    def _get_entry_id(entry):
        """è·å–æ¡ç›®å”¯ä¸€æ ‡è¯†"""
        return f"{entry['file_name']}_{entry['user']['screen_name']}_{entry['media_type']}"


# --------------------
# å‘½ä»¤è¡Œæ¥å£
# --------------------
def main():
    core = XBotCore()
    args = sys.argv[1:]  # è·å–å‘½ä»¤è¡Œå‚æ•°

    # æŒ‡å®šè¾“å‡ºç›®å½•ï¼špython X-Bot.py æ•°æ®æ–‡ä»¶ è¾“å‡ºæ–‡ä»¶
    if len(args) == 2:
        data_path = os.path.normpath(args[0])
        output_path = os.path.normpath(args[1])

        if os.path.exists(data_path):
            logger.info(f"ğŸ”§ è‡ªå®šä¹‰æ¨¡å¼å¤„ç†ï¼š{data_path}")
            core.process_single_day(data_path, output_path)
        else:
            logger.info(f"â­ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ•°æ®æ–‡ä»¶ï¼š{data_path}")

    # å•å‚æ•°æ¨¡å¼ï¼špython X-Bot.py æ•°æ®æ–‡ä»¶
    elif len(args) == 1:
        data_path = os.path.normpath(args[0])
        current_date = datetime.now()

        # ç”Ÿæˆå½“å¤©è¾“å‡ºè·¯å¾„
        output_dir = os.path.normpath(
            f"{Config.DEFAULT_OUTPUT_DIR}{current_date.strftime(Config.YEAR_MONTH)}/"
        )
        output_filename = f"{current_date.strftime(Config.YEAR_MONTH_DAY)}.json"
        output_path = os.path.join(output_dir, output_filename)

        if os.path.exists(data_path):
            logger.info(f"âš¡ å•æ–‡ä»¶æ¨¡å¼å¤„ç†ï¼š{os.path.basename(data_path)}")
            os.makedirs(output_dir, exist_ok=True)
            new_entries_count = core.process_single_day(data_path, output_path)
            # è¿”å›æ–°å¢æ¡æ•°
            print(new_entries_count)
        else:
            logger.info(f"â­ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ•°æ®æ–‡ä»¶ï¼š{data_path}")
            print(0)

    # æ— å‚æ•°æ¨¡å¼ï¼špython X-Bot.py
    elif len(args) == 0:
        current_date = datetime.now()

        logger.info("ğŸ¤– è‡ªåŠ¨æ¨¡å¼ï¼šå¤„ç†æœ€è¿‘ä¸€å‘¨æ•°æ®")
        for day_offset in reversed(range(8)):  # åŒ…å«ä»Šå¤©å…±8å¤©
            target_date = current_date - timedelta(days=day_offset)

            # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰æ•°æ®æ—¥æœŸï¼‰
            data_dir = os.path.normpath(
                f"{Config.DEFAULT_INPUT_DIR}{target_date.strftime(Config.YEAR_MONTH)}/"
            )
            data_filename = f"{target_date.strftime(Config.YEAR_MONTH_DAY)}.json"
            data_path = os.path.join(data_dir, data_filename)

            # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰æ•°æ®æ—¥æœŸï¼‰
            output_dir = os.path.normpath(
                f"{Config.DEFAULT_OUTPUT_DIR}{target_date.strftime(Config.YEAR_MONTH)}/"
            )
            output_path = os.path.join(output_dir, data_filename)

            if os.path.exists(data_path):
                logger.info(f"ğŸ” æ­£åœ¨å¤„ç† {target_date.strftime(Config.YEAR_MONTH_DAY)} æ•°æ®...")
                os.makedirs(output_dir, exist_ok=True)
                core.process_single_day(data_path, output_path)
            else:
                logger.info(f"â­ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ•°æ®æ–‡ä»¶ï¼š{data_filename}")

    # é”™è¯¯å‚æ•°å¤„ç†
    else:
        logger.error("â— å‚æ•°é”™è¯¯ï¼æ”¯æŒä»¥ä¸‹æ¨¡å¼ï¼š")
        logger.error("1. å…¨å‚æ•°æ¨¡å¼ï¼šè„šæœ¬ + æ•°æ®æ–‡ä»¶ + è¾“å‡ºæ–‡ä»¶")
        logger.error("2. å•æ–‡ä»¶æ¨¡å¼ï¼šè„šæœ¬ + æ•°æ®æ–‡ä»¶ï¼ˆè¾“å‡ºåˆ°å½“å¤©ç›®å½•ï¼‰")
        logger.error("3. è‡ªåŠ¨æ¨¡å¼ï¼šä»…è„šæœ¬ï¼ˆå¤„ç†æœ€è¿‘ä¸€å‘¨æ•°æ®ï¼‰")
        logger.error("ç¤ºä¾‹ï¼š")
        logger.error(
            "python X-Bot.py ../../TypeScript/tweets/2000-01/2000-01-01.json ../output/2000-01/2000-01-01.json")
        logger.error("python X-Bot.py ../../TypeScript/tweets/user/xxx.json")
        logger.error("python X-Bot.py")
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
        logger.info("ğŸ æ‰€æœ‰å¤„ç†ä»»åŠ¡å·²å®Œæˆï¼")
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ğŸ’¥ æœªå¤„ç†çš„å¼‚å¸¸: {str(e)}")
        sys.exit(1)
